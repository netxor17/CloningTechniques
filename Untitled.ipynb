{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668bb1bd-2582-4431-a4e6-60c3c953d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "######### face enhancement\n",
    "from face_parse.face_parsing import FaceParse\n",
    "from face_detect.retinaface_detection import RetinaFaceDetection\n",
    "from face_parse.face_parsing import FaceParse\n",
    "from face_model.face_gan import FaceGAN\n",
    "# from sr_model.real_esrnet import RealESRNet\n",
    "from align_faces import warp_and_crop_face, get_reference_facial_points\n",
    "from utils.inference_utils import Laplacian_Pyramid_Blending_with_mask\n",
    "\n",
    "class FaceEnhancement(object):\n",
    "    def __init__(self, base_dir='./', size=512, model=None, use_sr=True, sr_model=None, channel_multiplier=2, narrow=1, device='cuda'):\n",
    "        self.facedetector = RetinaFaceDetection(base_dir, device)\n",
    "        self.facegan = FaceGAN(base_dir, size, model, channel_multiplier, narrow, device=device)\n",
    "        # self.srmodel =  RealESRNet(base_dir, sr_model, device=device)\n",
    "        self.srmodel=None\n",
    "        self.faceparser = FaceParse(base_dir, device=device)\n",
    "        self.use_sr = use_sr\n",
    "        self.size = size\n",
    "        self.threshold = 0.9\n",
    "\n",
    "        # the mask for pasting restored faces back\n",
    "        self.mask = np.zeros((512, 512), np.float32)\n",
    "        cv2.rectangle(self.mask, (26, 26), (486, 486), (1, 1, 1), -1, cv2.LINE_AA)\n",
    "        self.mask = cv2.GaussianBlur(self.mask, (101, 101), 11)\n",
    "        self.mask = cv2.GaussianBlur(self.mask, (101, 101), 11)\n",
    "\n",
    "        self.kernel = np.array((\n",
    "                [0.0625, 0.125, 0.0625],\n",
    "                [0.125, 0.25, 0.125],\n",
    "                [0.0625, 0.125, 0.0625]), dtype=\"float32\")\n",
    "\n",
    "        # get the reference 5 landmarks position in the crop settings\n",
    "        default_square = True\n",
    "        inner_padding_factor = 0.25\n",
    "        outer_padding = (0, 0)\n",
    "        self.reference_5pts = get_reference_facial_points(\n",
    "                (self.size, self.size), inner_padding_factor, outer_padding, default_square)\n",
    "\n",
    "    def mask_postprocess(self, mask, thres=20):\n",
    "        mask[:thres, :] = 0; mask[-thres:, :] = 0\n",
    "        mask[:, :thres] = 0; mask[:, -thres:] = 0        \n",
    "        mask = cv2.GaussianBlur(mask, (101, 101), 11)\n",
    "        mask = cv2.GaussianBlur(mask, (101, 101), 11)\n",
    "        return mask.astype(np.float32)\n",
    "    \n",
    "    def process(self, img, ori_img, bbox=None, face_enhance=True, possion_blending=False):\n",
    "        if self.use_sr:\n",
    "            img_sr = self.srmodel.process(img)\n",
    "            if img_sr is not None:\n",
    "                img = cv2.resize(img, img_sr.shape[:2][::-1])\n",
    "\n",
    "        facebs, landms = self.facedetector.detect(img.copy())\n",
    "\n",
    "        orig_faces, enhanced_faces = [], []\n",
    "        height, width = img.shape[:2]\n",
    "        full_mask = np.zeros((height, width), dtype=np.float32)\n",
    "        mask_sharp = np.zeros((height, width), dtype=np.float32)\n",
    "        full_img = np.zeros(ori_img.shape, dtype=np.uint8)\n",
    "\n",
    "        for i, (faceb, facial5points) in enumerate(zip(facebs, landms)):\n",
    "            if faceb[4]<self.threshold: continue\n",
    "            fh, fw = (faceb[3]-faceb[1]), (faceb[2]-faceb[0])\n",
    "\n",
    "            facial5points = np.reshape(facial5points, (2, 5))\n",
    "\n",
    "            of, tfm_inv = warp_and_crop_face(img, facial5points, reference_pts=self.reference_5pts, crop_size=(self.size, self.size))\n",
    "\n",
    "            # enhance the face\n",
    "            if face_enhance:\n",
    "                ef = self.facegan.process(of)\n",
    "            else:\n",
    "                ef = of\n",
    "                    \n",
    "            orig_faces.append(of)\n",
    "            enhanced_faces.append(ef)\n",
    "            \n",
    "            # print(ef.shape)\n",
    "            # tmp_mask = self.mask\n",
    "            '''\n",
    "            0: 'background' 1: 'skin'   2: 'nose'\n",
    "            3: 'eye_g'  4: 'l_eye'  5: 'r_eye'\n",
    "            6: 'l_brow' 7: 'r_brow' 8: 'l_ear'\n",
    "            9: 'r_ear'  10: 'mouth' 11: 'u_lip'\n",
    "            12: 'l_lip' 13: 'hair'  14: 'hat'\n",
    "            15: 'ear_r' 16: 'neck_l'    17: 'neck'\n",
    "            18: 'cloth'\n",
    "            '''\n",
    "\n",
    "            # no ear, no neck, no hair&hat,  only face region\n",
    "            mm = [0, 255, 255, 255, 255, 255, 255, 255, 0, 0, 255, 255, 255, 0, 0, 0, 0, 0, 0]\n",
    "            mask_sharp = self.faceparser.process(ef, mm)[0]/255.\n",
    "            tmp_mask = self.mask_postprocess(mask_sharp)\n",
    "            tmp_mask = cv2.resize(tmp_mask, ef.shape[:2])\n",
    "            mask_sharp = cv2.resize(mask_sharp, ef.shape[:2])\n",
    "\n",
    "            tmp_mask = cv2.warpAffine(tmp_mask, tfm_inv, (width, height), flags=3)\n",
    "            mask_sharp = cv2.warpAffine(mask_sharp, tfm_inv, (width, height), flags=3)\n",
    "\n",
    "            if min(fh, fw)<100: # gaussian filter for small faces\n",
    "                ef = cv2.filter2D(ef, -1, self.kernel)\n",
    "            \n",
    "            if face_enhance:\n",
    "                tmp_img = cv2.warpAffine(ef, tfm_inv, (width, height), flags=3)\n",
    "            else:\n",
    "                tmp_img = cv2.warpAffine(of, tfm_inv, (width, height), flags=3)\n",
    "\n",
    "            mask = tmp_mask - full_mask\n",
    "            full_mask[np.where(mask>0)] = tmp_mask[np.where(mask>0)]\n",
    "            full_img[np.where(mask>0)] = tmp_img[np.where(mask>0)]\n",
    "\n",
    "        mask_sharp = cv2.GaussianBlur(mask_sharp, (0,0), sigmaX=1, sigmaY=1, borderType = cv2.BORDER_DEFAULT)\n",
    "\n",
    "        full_mask = full_mask[:, :, np.newaxis]\n",
    "        mask_sharp = mask_sharp[:, :, np.newaxis]\n",
    "\n",
    "        if self.use_sr and img_sr is not None:\n",
    "            img = cv2.convertScaleAbs(img_sr*(1-full_mask) + full_img*full_mask)\n",
    "        \n",
    "        elif possion_blending is True:\n",
    "            if bbox is not None:\n",
    "                y1, y2, x1, x2 = bbox\n",
    "                mask_bbox = np.zeros_like(mask_sharp)\n",
    "                mask_bbox[y1:y2 - 5, x1:x2] = 1\n",
    "                full_img, ori_img, full_mask = [cv2.resize(x,(512,512)) for x in (full_img, ori_img, np.float32(mask_sharp * mask_bbox))]\n",
    "            else:\n",
    "                full_img, ori_img, full_mask = [cv2.resize(x,(512,512)) for x in (full_img, ori_img, full_mask)]\n",
    "            \n",
    "            img = Laplacian_Pyramid_Blending_with_mask(full_img, ori_img, full_mask, 6)\n",
    "            img = np.clip(img, 0 ,255)\n",
    "            img = np.uint8(cv2.resize(img, (width, height)))\n",
    "\n",
    "        else:\n",
    "            img = cv2.convertScaleAbs(ori_img*(1-full_mask) + full_img*full_mask)\n",
    "            img = cv2.convertScaleAbs(ori_img*(1-mask_sharp) + img*mask_sharp)\n",
    "\n",
    "        return img, orig_faces, enhanced_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23877965-275b-4772-8d47-2ecebdfb0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2, os, sys, subprocess, platform, torch\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from scipy.io import loadmat\n",
    "\n",
    "sys.path.insert(0, 'third_part')\n",
    "sys.path.insert(0, 'third_part/GPEN')\n",
    "sys.path.insert(0, 'third_part/GFPGAN')\n",
    "\n",
    "# 3dmm extraction\n",
    "from third_part.face3d.util.preprocess import align_img\n",
    "from third_part.face3d.util.load_mats import load_lm3d\n",
    "from third_part.face3d.extract_kp_videos import KeypointExtractor\n",
    "# face enhancement\n",
    "from third_part.GPEN.gpen_face_enhancer import FaceEnhancement\n",
    "from third_part.GFPGAN.gfpgan import GFPGANer\n",
    "# expression control\n",
    "from third_part.ganimation_replicate.model.ganimation import GANimationModel\n",
    "\n",
    "from utils import audio\n",
    "from utils.ffhq_preprocess import Croper\n",
    "from utils.alignment_stit import crop_faces, calc_alignment_coefficients, paste_image\n",
    "from utils.inference_utils import Laplacian_Pyramid_Blending_with_mask, face_detect, load_model, options, split_coeff, \\\n",
    "                                  trans_image, transform_semantic, find_crop_norm_ratio, load_face3d_net, exp_aus_dict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "args = options()\n",
    "\n",
    "def main():    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print('[Info] Using {} for inference.'.format(device))\n",
    "    os.makedirs(os.path.join('temp', args.tmp_dir), exist_ok=True)\n",
    "\n",
    "    enhancer = FaceEnhancement(base_dir='checkpoints', size=512, model='GPEN-BFR-512', use_sr=False, \\\n",
    "                               sr_model='rrdb_realesrnet_psnr', channel_multiplier=2, narrow=1, device=device)\n",
    "    restorer = GFPGANer(model_path='checkpoints/GFPGANv1.3.pth', upscale=1, arch='clean', \\\n",
    "                        channel_multiplier=2, bg_upsampler=None)\n",
    "\n",
    "    base_name = args.face.split('/')[-1]\n",
    "    if os.path.isfile(args.face) and args.face.split('.')[1] in ['jpg', 'png', 'jpeg']:\n",
    "        args.static = True\n",
    "    if not os.path.isfile(args.face):\n",
    "        raise ValueError('--face argument must be a valid path to video/image file')\n",
    "    elif args.face.split('.')[1] in ['jpg', 'png', 'jpeg']:\n",
    "        full_frames = [cv2.imread(args.face)]\n",
    "        fps = args.fps\n",
    "    else:\n",
    "        video_stream = cv2.VideoCapture(args.face)\n",
    "        fps = video_stream.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        full_frames = []\n",
    "        while True:\n",
    "            still_reading, frame = video_stream.read()\n",
    "            if not still_reading:\n",
    "                video_stream.release()\n",
    "                break\n",
    "            y1, y2, x1, x2 = args.crop\n",
    "            if x2 == -1: x2 = frame.shape[1]\n",
    "            if y2 == -1: y2 = frame.shape[0]\n",
    "            frame = frame[y1:y2, x1:x2]\n",
    "            full_frames.append(frame)\n",
    "\n",
    "    print (\"[Step 0] Number of frames available for inference: \"+str(len(full_frames)))\n",
    "    # face detection & cropping, cropping the first frame as the style of FFHQ\n",
    "    croper = Croper('checkpoints/shape_predictor_68_face_landmarks.dat')\n",
    "    full_frames_RGB = [cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) for frame in full_frames]\n",
    "    full_frames_RGB, crop, quad = croper.crop(full_frames_RGB, xsize=512)\n",
    "\n",
    "    clx, cly, crx, cry = crop\n",
    "    lx, ly, rx, ry = quad\n",
    "    lx, ly, rx, ry = int(lx), int(ly), int(rx), int(ry)\n",
    "    oy1, oy2, ox1, ox2 = cly+ly, min(cly+ry, full_frames[0].shape[0]), clx+lx, min(clx+rx, full_frames[0].shape[1])\n",
    "    # original_size = (ox2 - ox1, oy2 - oy1)\n",
    "    frames_pil = [Image.fromarray(cv2.resize(frame,(256,256))) for frame in full_frames_RGB]\n",
    "\n",
    "    # get the landmark according to the detected face.\n",
    "    if not os.path.isfile('temp/'+base_name+'_landmarks.txt') or args.re_preprocess:\n",
    "        print('[Step 1] Landmarks Extraction in Video.')\n",
    "        kp_extractor = KeypointExtractor()\n",
    "        lm = kp_extractor.extract_keypoint(frames_pil, './temp/'+base_name+'_landmarks.txt')\n",
    "    else:\n",
    "        print('[Step 1] Using saved landmarks.')\n",
    "        lm = np.loadtxt('temp/'+base_name+'_landmarks.txt').astype(np.float32)\n",
    "        lm = lm.reshape([len(full_frames), -1, 2])\n",
    "       \n",
    "    if not os.path.isfile('temp/'+base_name+'_coeffs.npy') or args.exp_img is not None or args.re_preprocess:\n",
    "        net_recon = load_face3d_net(args.face3d_net_path, device)\n",
    "        lm3d_std = load_lm3d('checkpoints/BFM')\n",
    "\n",
    "        video_coeffs = []\n",
    "        for idx in tqdm(range(len(frames_pil)), desc=\"[Step 2] 3DMM Extraction In Video:\"):\n",
    "            frame = frames_pil[idx]\n",
    "            W, H = frame.size\n",
    "            lm_idx = lm[idx].reshape([-1, 2])\n",
    "            if np.mean(lm_idx) == -1:\n",
    "                lm_idx = (lm3d_std[:, :2]+1) / 2.\n",
    "                lm_idx = np.concatenate([lm_idx[:, :1] * W, lm_idx[:, 1:2] * H], 1)\n",
    "            else:\n",
    "                lm_idx[:, -1] = H - 1 - lm_idx[:, -1]\n",
    "\n",
    "            trans_params, im_idx, lm_idx, _ = align_img(frame, lm_idx, lm3d_std)\n",
    "            trans_params = np.array([float(item) for item in np.hsplit(trans_params, 5)]).astype(np.float32)\n",
    "            im_idx_tensor = torch.tensor(np.array(im_idx)/255., dtype=torch.float32).permute(2, 0, 1).to(device).unsqueeze(0) \n",
    "            with torch.no_grad():\n",
    "                coeffs = split_coeff(net_recon(im_idx_tensor))\n",
    "\n",
    "            pred_coeff = {key:coeffs[key].cpu().numpy() for key in coeffs}\n",
    "            pred_coeff = np.concatenate([pred_coeff['id'], pred_coeff['exp'], pred_coeff['tex'], pred_coeff['angle'],\\\n",
    "                                         pred_coeff['gamma'], pred_coeff['trans'], trans_params[None]], 1)\n",
    "            video_coeffs.append(pred_coeff)\n",
    "        semantic_npy = np.array(video_coeffs)[:,0]\n",
    "        np.save('temp/'+base_name+'_coeffs.npy', semantic_npy)\n",
    "    else:\n",
    "        print('[Step 2] Using saved coeffs.')\n",
    "        semantic_npy = np.load('temp/'+base_name+'_coeffs.npy').astype(np.float32)\n",
    "\n",
    "    # generate the 3dmm coeff from a single image\n",
    "    if args.exp_img is not None and ('.png' in args.exp_img or '.jpg' in args.exp_img):\n",
    "        print('extract the exp from',args.exp_img)\n",
    "        exp_pil = Image.open(args.exp_img).convert('RGB')\n",
    "        lm3d_std = load_lm3d('third_part/face3d/BFM')\n",
    "        \n",
    "        W, H = exp_pil.size\n",
    "        kp_extractor = KeypointExtractor()\n",
    "        lm_exp = kp_extractor.extract_keypoint([exp_pil], 'temp/'+base_name+'_temp.txt')[0]\n",
    "        if np.mean(lm_exp) == -1:\n",
    "            lm_exp = (lm3d_std[:, :2] + 1) / 2.\n",
    "            lm_exp = np.concatenate(\n",
    "                [lm_exp[:, :1] * W, lm_exp[:, 1:2] * H], 1)\n",
    "        else:\n",
    "            lm_exp[:, -1] = H - 1 - lm_exp[:, -1]\n",
    "\n",
    "        trans_params, im_exp, lm_exp, _ = align_img(exp_pil, lm_exp, lm3d_std)\n",
    "        trans_params = np.array([float(item) for item in np.hsplit(trans_params, 5)]).astype(np.float32)\n",
    "        im_exp_tensor = torch.tensor(np.array(im_exp)/255., dtype=torch.float32).permute(2, 0, 1).to(device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            expression = split_coeff(net_recon(im_exp_tensor))['exp'][0]\n",
    "        del net_recon\n",
    "    elif args.exp_img == 'smile':\n",
    "        expression = torch.tensor(loadmat('checkpoints/expression.mat')['expression_mouth'])[0]\n",
    "    else:\n",
    "        print('using expression center')\n",
    "        expression = torch.tensor(loadmat('checkpoints/expression.mat')['expression_center'])[0]\n",
    "\n",
    "    # load DNet, model(LNet and ENet)\n",
    "    D_Net, model = load_model(args, device)\n",
    "\n",
    "    if not os.path.isfile('temp/'+base_name+'_stablized.npy') or args.re_preprocess:\n",
    "        imgs = []\n",
    "        for idx in tqdm(range(len(frames_pil)), desc=\"[Step 3] Stabilize the expression In Video:\"):\n",
    "            if args.one_shot:\n",
    "                source_img = trans_image(frames_pil[0]).unsqueeze(0).to(device)\n",
    "                semantic_source_numpy = semantic_npy[0:1]\n",
    "            else:\n",
    "                source_img = trans_image(frames_pil[idx]).unsqueeze(0).to(device)\n",
    "                semantic_source_numpy = semantic_npy[idx:idx+1]\n",
    "            ratio = find_crop_norm_ratio(semantic_source_numpy, semantic_npy)\n",
    "            coeff = transform_semantic(semantic_npy, idx, ratio).unsqueeze(0).to(device)\n",
    "        \n",
    "            # hacking the new expression\n",
    "            coeff[:, :64, :] = expression[None, :64, None].to(device) \n",
    "            with torch.no_grad():\n",
    "                output = D_Net(source_img, coeff)\n",
    "            img_stablized = np.uint8((output['fake_image'].squeeze(0).permute(1,2,0).cpu().clamp_(-1, 1).numpy() + 1 )/2. * 255)\n",
    "            imgs.append(cv2.cvtColor(img_stablized,cv2.COLOR_RGB2BGR)) \n",
    "        np.save('temp/'+base_name+'_stablized.npy',imgs)\n",
    "        del D_Net\n",
    "    else:\n",
    "        print('[Step 3] Using saved stabilized video.')\n",
    "        imgs = np.load('temp/'+base_name+'_stablized.npy')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    if not args.audio.endswith('.wav'):\n",
    "        command = 'ffmpeg -loglevel error -y -i {} -strict -2 {}'.format(args.audio, 'temp/{}/temp.wav'.format(args.tmp_dir))\n",
    "        subprocess.call(command, shell=True)\n",
    "        args.audio = 'temp/{}/temp.wav'.format(args.tmp_dir)\n",
    "    wav = audio.load_wav(args.audio, 16000)\n",
    "    mel = audio.melspectrogram(wav)\n",
    "    if np.isnan(mel.reshape(-1)).sum() > 0:\n",
    "        raise ValueError('Mel contains nan! Using a TTS voice? Add a small epsilon noise to the wav file and try again')\n",
    "\n",
    "    mel_step_size, mel_idx_multiplier, i, mel_chunks = 16, 80./fps, 0, []\n",
    "    while True:\n",
    "        start_idx = int(i * mel_idx_multiplier)\n",
    "        if start_idx + mel_step_size > len(mel[0]):\n",
    "            mel_chunks.append(mel[:, len(mel[0]) - mel_step_size:])\n",
    "            break\n",
    "        mel_chunks.append(mel[:, start_idx : start_idx + mel_step_size])\n",
    "        i += 1\n",
    "\n",
    "    print(\"[Step 4] Load audio; Length of mel chunks: {}\".format(len(mel_chunks)))\n",
    "    imgs = imgs[:len(mel_chunks)]\n",
    "    full_frames = full_frames[:len(mel_chunks)]  \n",
    "    lm = lm[:len(mel_chunks)]\n",
    "    \n",
    "    imgs_enhanced = []\n",
    "    for idx in tqdm(range(len(imgs)), desc='[Step 5] Reference Enhancement'):\n",
    "        img = imgs[idx]\n",
    "        pred, _, _ = enhancer.process(img, img, face_enhance=True, possion_blending=False)\n",
    "        imgs_enhanced.append(pred)\n",
    "    gen = datagen(imgs_enhanced.copy(), mel_chunks, full_frames, None, (oy1,oy2,ox1,ox2))\n",
    "\n",
    "    frame_h, frame_w = full_frames[0].shape[:-1]\n",
    "    out = cv2.VideoWriter('temp/{}/result.mp4'.format(args.tmp_dir), cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_w, frame_h))\n",
    "    \n",
    "    if args.up_face != 'original':\n",
    "        instance = GANimationModel()\n",
    "        instance.initialize()\n",
    "        instance.setup()\n",
    "\n",
    "    kp_extractor = KeypointExtractor()\n",
    "    for i, (img_batch, mel_batch, frames, coords, img_original, f_frames) in enumerate(tqdm(gen, desc='[Step 6] Lip Synthesis:', total=int(np.ceil(float(len(mel_chunks)) / args.LNet_batch_size)))):\n",
    "        img_batch = torch.FloatTensor(np.transpose(img_batch, (0, 3, 1, 2))).to(device)\n",
    "        mel_batch = torch.FloatTensor(np.transpose(mel_batch, (0, 3, 1, 2))).to(device)\n",
    "        img_original = torch.FloatTensor(np.transpose(img_original, (0, 3, 1, 2))).to(device)/255. # BGR -> RGB\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            incomplete, reference = torch.split(img_batch, 3, dim=1) \n",
    "            pred, low_res = model(mel_batch, img_batch, reference)\n",
    "            pred = torch.clamp(pred, 0, 1)\n",
    "\n",
    "            if args.up_face in ['sad', 'angry', 'surprise']:\n",
    "                tar_aus = exp_aus_dict[args.up_face]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            if args.up_face == 'original':\n",
    "                cur_gen_faces = img_original\n",
    "            else:\n",
    "                test_batch = {'src_img': torch.nn.functional.interpolate((img_original * 2 - 1), size=(128, 128), mode='bilinear'), \n",
    "                              'tar_aus': tar_aus.repeat(len(incomplete), 1)}\n",
    "                instance.feed_batch(test_batch)\n",
    "                instance.forward()\n",
    "                cur_gen_faces = torch.nn.functional.interpolate(instance.fake_img / 2. + 0.5, size=(384, 384), mode='bilinear')\n",
    "                \n",
    "            if args.without_rl1 is not False:\n",
    "                incomplete, reference = torch.split(img_batch, 3, dim=1)\n",
    "                mask = torch.where(incomplete==0, torch.ones_like(incomplete), torch.zeros_like(incomplete)) \n",
    "                pred = pred * mask + cur_gen_faces * (1 - mask) \n",
    "        \n",
    "        pred = pred.cpu().numpy().transpose(0, 2, 3, 1) * 255.\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        for p, f, xf, c in zip(pred, frames, f_frames, coords):\n",
    "            if len(c) == 0:\n",
    "                out.write(f)\n",
    "                \n",
    "            else:\n",
    "                y1, y2, x1, x2 = c\n",
    "                p = cv2.resize(p.astype(np.uint8), (x2 - x1, y2 - y1))\n",
    "\n",
    "                ff = xf.copy() \n",
    "                ff[y1:y2, x1:x2] = p\n",
    "\n",
    "                # month region enhancement by GFPGAN\n",
    "                cropped_faces, restored_faces, restored_img = restorer.enhance(\n",
    "                    ff, has_aligned=False, only_center_face=True, paste_back=True)\n",
    "                    # 0,   1,   2,   3,   4,   5,   6,   7,   8,  9, 10,  11,  12,\n",
    "                mm = [0,   0,   0,   0,   0,   0,   0,   0,   0,  0, 255, 255, 255, 0, 0, 0, 0, 0, 0]\n",
    "                mouse_mask = np.zeros_like(restored_img)\n",
    "                tmp_mask = enhancer.faceparser.process(restored_img[y1:y2, x1:x2], mm)[0]\n",
    "                mouse_mask[y1:y2, x1:x2]= cv2.resize(tmp_mask, (x2 - x1, y2 - y1))[:, :, np.newaxis] / 255.\n",
    "\n",
    "                height, width = ff.shape[:2]\n",
    "                restored_img, ff, full_mask = [cv2.resize(x, (512, 512)) for x in (restored_img, ff, np.float32(mouse_mask))]\n",
    "                img = Laplacian_Pyramid_Blending_with_mask(restored_img, ff, full_mask[:, :, 0], 10)\n",
    "                pp = np.uint8(cv2.resize(np.clip(img, 0 ,255), (width, height)))\n",
    "\n",
    "                pp, orig_faces, enhanced_faces = enhancer.process(pp, xf, bbox=c, face_enhance=False, possion_blending=True)\n",
    "                out.write(pp)\n",
    "    out.release()\n",
    "    \n",
    "    if not os.path.isdir(os.path.dirname(args.outfile)):\n",
    "        os.makedirs(os.path.dirname(args.outfile), exist_ok=True)\n",
    "    command = 'ffmpeg -loglevel error -y -i {} -i {} -strict -2 -q:v 1 {}'.format(args.audio, 'temp/{}/result.mp4'.format(args.tmp_dir), args.outfile)\n",
    "    subprocess.call(command, shell=platform.system() != 'Windows')\n",
    "    print('outfile:', args.outfile)\n",
    "\n",
    "\n",
    "# frames:256x256, full_frames: original size\n",
    "# def datagen(frames, mels, full_frames, frames_pil, cox):\n",
    "#     img_batch, mel_batch, frame_batch, coords_batch, ref_batch, full_frame_batch = [], [], [], [], [], []\n",
    "#     base_name = args.face.split('/')[-1]\n",
    "#     refs = []\n",
    "#     image_size = 256 \n",
    "\n",
    "#     # original frames\n",
    "#     kp_extractor = KeypointExtractor()\n",
    "#     fr_pil = [Image.fromarray(frame) for frame in frames]\n",
    "#     lms = kp_extractor.extract_keypoint(fr_pil, 'temp/'+base_name+'x12_landmarks.txt')\n",
    "#     frames_pil = [ (lm, frame) for frame,lm in zip(fr_pil, lms)] # frames is the croped version of modified face\n",
    "#     crops, orig_images, quads  = crop_faces(image_size, frames_pil, scale=1.0, use_fa=True)\n",
    "#     inverse_transforms = [calc_alignment_coefficients(quad + 0.5, [[0, 0], [0, image_size], [image_size, image_size], [image_size, 0]]) for quad in quads]\n",
    "#     del kp_extractor.detector\n",
    "\n",
    "#     oy1,oy2,ox1,ox2 = cox\n",
    "#     face_det_results = face_detect(full_frames, args, jaw_correction=True)\n",
    "\n",
    "#     for inverse_transform, crop, full_frame, face_det in zip(inverse_transforms, crops, full_frames, face_det_results):\n",
    "#         imc_pil = paste_image(inverse_transform, crop, Image.fromarray(\n",
    "#             cv2.resize(full_frame[int(oy1):int(oy2), int(ox1):int(ox2)], (256, 256))))\n",
    "\n",
    "#         ff = full_frame.copy()\n",
    "#         ff[int(oy1):int(oy2), int(ox1):int(ox2)] = cv2.resize(np.array(imc_pil.convert('RGB')), (ox2 - ox1, oy2 - oy1))\n",
    "#         oface, coords = face_det\n",
    "#         y1, y2, x1, x2 = coords\n",
    "#         refs.append(ff[y1: y2, x1:x2])\n",
    "\n",
    "#     for i, m in enumerate(mels):\n",
    "#         idx = 0 if args.static else i % len(frames)\n",
    "#         frame_to_save = frames[idx].copy()\n",
    "#         face = refs[idx]\n",
    "#         oface, coords = face_det_results[idx].copy()\n",
    "        \n",
    "        \n",
    "#         #Code added for solving the error \"No face detected\"\n",
    "#         if len(coords) == 0:\n",
    "#             face = np.random.rand(args.img_size, args.img_size,3) * 255\n",
    "#             oface = np.random.rand(args.img_size, args.img_size,3) * 255\n",
    "#         else:\n",
    "#             face = cv2.resize(face, (args.img_size, args.img_size))\n",
    "#             oface = cv2.resize(oface, (args.img_size, args.img_size))\n",
    "\n",
    "#         img_batch.append(oface)\n",
    "#         ref_batch.append(face) \n",
    "#         mel_batch.append(m)\n",
    "#         coords_batch.append(coords)\n",
    "#         frame_batch.append(frame_to_save)\n",
    "#         full_frame_batch.append(full_frames[idx].copy())\n",
    "\n",
    "#         if len(img_batch) >= args.LNet_batch_size:\n",
    "#             img_batch, mel_batch, ref_batch = np.asarray(img_batch), np.asarray(mel_batch), np.asarray(ref_batch)\n",
    "#             img_masked = img_batch.copy()\n",
    "#             img_original = img_batch.copy()\n",
    "#             img_masked[:, args.img_size//2:] = 0\n",
    "#             img_batch = np.concatenate((img_masked, ref_batch), axis=3) / 255.\n",
    "#             mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
    "\n",
    "#             yield img_batch, mel_batch, frame_batch, coords_batch, img_original, full_frame_batch\n",
    "#             img_batch, mel_batch, frame_batch, coords_batch, img_original, full_frame_batch, ref_batch  = [], [], [], [], [], [], []\n",
    "\n",
    "#     if len(img_batch) > 0:\n",
    "#         img_batch, mel_batch, ref_batch = np.asarray(img_batch), np.asarray(mel_batch), np.asarray(ref_batch)\n",
    "#         img_masked = img_batch.copy()\n",
    "#         img_original = img_batch.copy()\n",
    "#         img_masked[:, args.img_size//2:] = 0\n",
    "#         img_batch = np.concatenate((img_masked, ref_batch), axis=3) / 255.\n",
    "#         mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
    "#         yield img_batch, mel_batch, frame_batch, coords_batch, img_original, full_frame_batch\n",
    "\n",
    "\n",
    "def datagen(frames, mels, full_frames, frames_pil, cox):\n",
    "    img_batch, mel_batch, frame_batch, coords_batch, ref_batch, full_frame_batch = [], [], [], [], [], []\n",
    "    base_name = args.face.split('/')[-1]\n",
    "    refs = []\n",
    "    image_size = 256 \n",
    "\n",
    "    # original frames\n",
    "    kp_extractor = KeypointExtractor()\n",
    "    fr_pil = [Image.fromarray(frame) for frame in frames]\n",
    "    lms = kp_extractor.extract_keypoint(fr_pil, 'temp/'+base_name+'x12_landmarks.txt')\n",
    "    frames_pil = [(lm, frame) for frame, lm in zip(fr_pil, lms)] # frames is the cropped version of modified face\n",
    "    crops, orig_images, quads = crop_faces(image_size, frames_pil, scale=1.0, use_fa=True)\n",
    "    inverse_transforms = [calc_alignment_coefficients(quad + 0.5, [[0, 0], [0, image_size], [image_size, image_size], [image_size, 0]]) for quad in quads]\n",
    "    del kp_extractor.detector\n",
    "\n",
    "    oy1, oy2, ox1, ox2 = cox\n",
    "    face_det_results = face_detect(full_frames, args, jaw_correction=True)\n",
    "\n",
    "    for inverse_transform, crop, full_frame, face_det in zip(inverse_transforms, crops, full_frames, face_det_results):\n",
    "        imc_pil = paste_image(inverse_transform, crop, Image.fromarray(\n",
    "            cv2.resize(full_frame[int(oy1):int(oy2), int(ox1):int(ox2)], (256, 256))))\n",
    "\n",
    "        ff = full_frame.copy()\n",
    "        ff[int(oy1):int(oy2), int(ox1):int(ox2)] = cv2.resize(np.array(imc_pil.convert('RGB')), (ox2 - ox1, oy2 - oy1))\n",
    "        oface, coords = face_det\n",
    "        y1, y2, x1, x2 = coords\n",
    "        refs.append(ff[y1: y2, x1:x2])\n",
    "\n",
    "    for i, m in enumerate(mels):\n",
    "        idx = 0 if args.static else i % len(frames)\n",
    "        frame_to_save = frames[idx].copy()\n",
    "\n",
    "        # Verificar si idx está dentro de los límites de refs\n",
    "        if idx < len(refs):\n",
    "            face = refs[idx]\n",
    "            oface, coords = face_det_results[idx].copy()\n",
    "\n",
    "            face = cv2.resize(face, (args.img_size, args.img_size))\n",
    "            oface = cv2.resize(oface, (args.img_size, args.img_size))\n",
    "\n",
    "            img_batch.append(oface)\n",
    "            ref_batch.append(face) \n",
    "            mel_batch.append(m)\n",
    "            coords_batch.append(coords)\n",
    "            frame_batch.append(frame_to_save)\n",
    "            full_frame_batch.append(full_frames[idx].copy())\n",
    "        else:\n",
    "            # Si idx está fuera de los límites de refs, mantener la imagen original\n",
    "            img_batch.append(frames[idx])\n",
    "            ref_batch.append(frames[idx])\n",
    "            mel_batch.append(m)\n",
    "            coords_batch.append([])\n",
    "            frame_batch.append(frame_to_save)\n",
    "            full_frame_batch.append(full_frames[idx].copy())\n",
    "\n",
    "        if len(img_batch) >= args.LNet_batch_size:\n",
    "            img_batch, mel_batch, ref_batch = np.asarray(img_batch), np.asarray(mel_batch), np.asarray(ref_batch)\n",
    "            img_masked = img_batch.copy()\n",
    "            img_original = img_batch.copy()\n",
    "            img_masked[:, args.img_size//2:] = 0\n",
    "            img_batch = np.concatenate((img_masked, ref_batch), axis=3) / 255.\n",
    "            mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
    "\n",
    "            yield img_batch, mel_batch, frame_batch, coords_batch, img_original, full_frame_batch\n",
    "            img_batch, mel_batch, frame_batch, coords_batch, img_original, full_frame_batch, ref_batch = [], [], [], [], [], [], []\n",
    "\n",
    "    if len(img_batch) > 0:\n",
    "        img_batch, mel_batch, ref_batch = np.asarray(img_batch), np.asarray(mel_batch), np.asarray(ref_batch)\n",
    "        img_masked = img_batch.copy()\n",
    "        img_original = img_batch.copy()\n",
    "        img_masked[:, args.img_size//2:] = 0\n",
    "        img_batch = np.concatenate((img_masked, ref_batch), axis=3) / 255.\n",
    "        mel_batch = np.reshape(mel_batch, [len(mel_batch), mel_batch.shape[1], mel_batch.shape[2], 1])\n",
    "\n",
    "        yield img_batch, mel_batch, frame_batch, coords_batch, img_original, full_frame_batch\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1cdf78-6b42-489c-90b4-0f4c4214d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2, argparse, torch\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from models import load_network, load_DNet\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from scipy.spatial import ConvexHull\n",
    "from third_part import face_detection\n",
    "from third_part.face3d.models import networks\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def options():\n",
    "    parser = argparse.ArgumentParser(description='Inference code to lip-sync videos in the wild using Wav2Lip models')\n",
    "\n",
    "    parser.add_argument('--DNet_path', type=str, default='checkpoints/DNet.pt')\n",
    "    parser.add_argument('--LNet_path', type=str, default='checkpoints/LNet.pth')\n",
    "    parser.add_argument('--ENet_path', type=str, default='checkpoints/ENet.pth') \n",
    "    parser.add_argument('--face3d_net_path', type=str, default='checkpoints/face3d_pretrain_epoch_20.pth')                      \n",
    "    parser.add_argument('--face', type=str, help='Filepath of video/image that contains faces to use', required=True)\n",
    "    parser.add_argument('--audio', type=str, help='Filepath of video/audio file to use as raw audio source', required=True)\n",
    "    parser.add_argument('--exp_img', type=str, help='Expression template. neutral, smile or image path', default='neutral')\n",
    "    parser.add_argument('--outfile', type=str, help='Video path to save result')\n",
    "\n",
    "    parser.add_argument('--fps', type=float, help='Can be specified only if input is a static image (default: 25)', default=25., required=False)\n",
    "    parser.add_argument('--pads', nargs='+', type=int, default=[0, 20, 0, 0], help='Padding (top, bottom, left, right). Please adjust to include chin at least')\n",
    "    parser.add_argument('--face_det_batch_size', type=int, help='Batch size for face detection', default=4)\n",
    "    parser.add_argument('--LNet_batch_size', type=int, help='Batch size for LNet', default=16)\n",
    "    parser.add_argument('--img_size', type=int, default=384)\n",
    "    parser.add_argument('--crop', nargs='+', type=int, default=[0, -1, 0, -1], \n",
    "                        help='Crop video to a smaller region (top, bottom, left, right). Applied after resize_factor and rotate arg. ' \n",
    "                        'Useful if multiple face present. -1 implies the value will be auto-inferred based on height, width')\n",
    "    parser.add_argument('--box', nargs='+', type=int, default=[-1, -1, -1, -1], \n",
    "                        help='Specify a constant bounding box for the face. Use only as a last resort if the face is not detected.'\n",
    "                        'Also, might work only if the face is not moving around much. Syntax: (top, bottom, left, right).')\n",
    "    parser.add_argument('--nosmooth', default=False, action='store_true', help='Prevent smoothing face detections over a short temporal window')\n",
    "    parser.add_argument('--static', default=False, action='store_true')\n",
    "\n",
    "    \n",
    "    parser.add_argument('--up_face', default='original')\n",
    "    parser.add_argument('--one_shot', action='store_true')\n",
    "    parser.add_argument('--without_rl1', default=False, action='store_true', help='Do not use the relative l1')\n",
    "    parser.add_argument('--tmp_dir', type=str, default='temp', help='Folder to save tmp results')\n",
    "    parser.add_argument('--re_preprocess', action='store_true')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "exp_aus_dict = {        # AU01_r, AU02_r, AU04_r, AU05_r, AU06_r, AU07_r, AU09_r, AU10_r, AU12_r, AU14_r, AU15_r, AU17_r, AU20_r, AU23_r, AU25_r, AU26_r, AU45_r.\n",
    "    'sad': torch.Tensor([[ 0,     0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0]]),\n",
    "    'angry':torch.Tensor([[0,     0,      0.3,    0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0]]),\n",
    "    'surprise': torch.Tensor([[0, 0,      0,      0.2,    0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0,      0]])\n",
    "}\n",
    "\n",
    "def mask_postprocess(mask, thres=20):\n",
    "    mask[:thres, :] = 0; mask[-thres:, :] = 0\n",
    "    mask[:, :thres] = 0; mask[:, -thres:] = 0\n",
    "    mask = cv2.GaussianBlur(mask, (101, 101), 11)\n",
    "    mask = cv2.GaussianBlur(mask, (101, 101), 11)\n",
    "    return mask.astype(np.float32)\n",
    "\n",
    "def trans_image(image):\n",
    "    image = TF.resize(\n",
    "        image, size=256, interpolation=Image.BICUBIC)\n",
    "    image = TF.to_tensor(image)\n",
    "    image = TF.normalize(image, mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    return image\n",
    "\n",
    "def obtain_seq_index(index, num_frames):\n",
    "    seq = list(range(index-13, index+13))\n",
    "    seq = [ min(max(item, 0), num_frames-1) for item in seq ]\n",
    "    return seq\n",
    "\n",
    "def transform_semantic(semantic, frame_index, crop_norm_ratio=None):\n",
    "    index = obtain_seq_index(frame_index, semantic.shape[0])\n",
    "    \n",
    "    coeff_3dmm = semantic[index,...]\n",
    "    ex_coeff = coeff_3dmm[:,80:144] #expression # 64\n",
    "    angles = coeff_3dmm[:,224:227] #euler angles for pose\n",
    "    translation = coeff_3dmm[:,254:257] #translation\n",
    "    crop = coeff_3dmm[:,259:262] #crop param\n",
    "\n",
    "    if crop_norm_ratio:\n",
    "        crop[:, -3] = crop[:, -3] * crop_norm_ratio\n",
    "\n",
    "    coeff_3dmm = np.concatenate([ex_coeff, angles, translation, crop], 1)\n",
    "    return torch.Tensor(coeff_3dmm).permute(1,0)   \n",
    "\n",
    "def find_crop_norm_ratio(source_coeff, target_coeffs):\n",
    "    alpha = 0.3\n",
    "    exp_diff = np.mean(np.abs(target_coeffs[:,80:144] - source_coeff[:,80:144]), 1) # mean different exp\n",
    "    angle_diff = np.mean(np.abs(target_coeffs[:,224:227] - source_coeff[:,224:227]), 1) # mean different angle\n",
    "    index = np.argmin(alpha*exp_diff + (1-alpha)*angle_diff)  # find the smallerest index\n",
    "    crop_norm_ratio = source_coeff[:,-3] / target_coeffs[index:index+1, -3]\n",
    "    return crop_norm_ratio\n",
    "\n",
    "# def get_smoothened_boxes(boxes, T):\n",
    "#     smoothed_boxes = []\n",
    "#     for i in range(len(boxes)):\n",
    "#         if i + T > len(boxes):\n",
    "#             window = boxes[len(boxes) - T:]\n",
    "#         else:\n",
    "#             window = boxes[i : i + T]\n",
    "        \n",
    "#         # Convert each element in the window to a NumPy array\n",
    "#         window = [np.array(box, dtype=np.float64) for box in window]\n",
    "#         # Calculate the mean of the window and append it to the smoothed_boxes list\n",
    "#         smoothed_boxes.append(np.mean(window, axis=0))\n",
    "#     return smoothed_boxes\n",
    "\n",
    "def get_smoothened_boxes(boxes, T):\n",
    "    smoothed_boxes = []\n",
    "    for i in range(len(boxes)):\n",
    "        if i + T > len(boxes):\n",
    "            window = boxes[len(boxes) - T:]\n",
    "        else:\n",
    "            window = boxes[i : i + T]\n",
    "        \n",
    "        # Convert each element in the window to a NumPy array\n",
    "        smoothed_window = []\n",
    "        for box in window:\n",
    "            try:\n",
    "                smoothed_window.append(np.array(box, dtype=np.float64))\n",
    "            except ValueError:\n",
    "                # Handle the case where an element cannot be converted to a NumPy array\n",
    "                smoothed_window.append(np.zeros_like(boxes[0]))  # Placeholder value\n",
    "        \n",
    "        # Check if each element in smoothed_window is a tuple before adding it to smoothed_boxes\n",
    "        for item in smoothed_window:\n",
    "            if isinstance(item, tuple):\n",
    "                smoothed_boxes.append(item)\n",
    "                \n",
    "    # Return the smoothed_boxes in the format [(image, (y1, y2, x1, x2)), ...]\n",
    "    return smoothed_boxes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def face_detect(images, args, jaw_correction=False, detector=None):\n",
    "    if detector == None:\n",
    "        device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "        detector = face_detection.FaceAlignment(face_detection.LandmarksType._2D, \n",
    "                                                flip_input=False, device=device)\n",
    "\n",
    "    batch_size = args.face_det_batch_size    \n",
    "    while 1:\n",
    "        predictions = []\n",
    "        try:\n",
    "            for i in tqdm(range(0, len(images), batch_size),desc='FaceDet:'):\n",
    "                predictions.extend(detector.get_detections_for_batch(np.array(images[i:i + batch_size])))\n",
    "        except RuntimeError:\n",
    "            if batch_size == 1: \n",
    "                raise RuntimeError('Image too big to run face detection on GPU. Please use the --resize_factor argument')\n",
    "            batch_size //= 2\n",
    "            print('Recovering from OOM error; New batch size: {}'.format(batch_size))\n",
    "            continue\n",
    "        break\n",
    "\n",
    "    results = []\n",
    "    pady1, pady2, padx1, padx2 = args.pads if jaw_correction else (0,20,0,0)\n",
    "    for rect, image in zip(predictions, images):\n",
    "        if rect is None:\n",
    "            #Changed code for avoiding the \"Face not detected!\" error\n",
    "            #cv2.imwrite('temp/faulty_frame.jpg', image) # check this frame where the face was not detected.\n",
    "            #raise ValueError('Face not detected! Ensure the video contains a face in all the frames.')\n",
    "            results.append([image, []])\n",
    "        else:\n",
    "            y1 = max(0, rect[1] - pady1)\n",
    "            y2 = min(image.shape[0], rect[3] + pady2)\n",
    "            x1 = max(0, rect[0] - padx1)\n",
    "            x2 = min(image.shape[1], rect[2] + padx2)\n",
    "            results.append([image[y1: y2, x1:x2], (y1, y2, x1, x2)])\n",
    "\n",
    "    boxes = np.array(results)\n",
    "    if not args.nosmooth:\n",
    "        boxes = get_smoothened_boxes(boxes, T=5)\n",
    "    \n",
    "    results = [[image[y1: y2, x1:x2], (y1, y2, x1, x2)] if rect is not None else [image, None] for image, (x1, y1, x2, y2) in zip(images, boxes)]\n",
    "\n",
    "    del detector\n",
    "    torch.cuda.empty_cache()\n",
    "    return results\n",
    "\n",
    "def _load(checkpoint_path, device):\n",
    "    if device == 'cuda':\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint\n",
    "\n",
    "def split_coeff(coeffs):\n",
    "        \"\"\"\n",
    "        Return:\n",
    "            coeffs_dict     -- a dict of torch.tensors\n",
    "\n",
    "        Parameters:\n",
    "            coeffs          -- torch.tensor, size (B, 256)\n",
    "        \"\"\"\n",
    "        id_coeffs = coeffs[:, :80]\n",
    "        exp_coeffs = coeffs[:, 80: 144]\n",
    "        tex_coeffs = coeffs[:, 144: 224]\n",
    "        angles = coeffs[:, 224: 227]\n",
    "        gammas = coeffs[:, 227: 254]\n",
    "        translations = coeffs[:, 254:]\n",
    "        return {\n",
    "            'id': id_coeffs,\n",
    "            'exp': exp_coeffs,\n",
    "            'tex': tex_coeffs,\n",
    "            'angle': angles,\n",
    "            'gamma': gammas,\n",
    "            'trans': translations\n",
    "        }\n",
    "\n",
    "def Laplacian_Pyramid_Blending_with_mask(A, B, m, num_levels = 6):\n",
    "    # generate Gaussian pyramid for A,B and mask\n",
    "    GA = A.copy()\n",
    "    GB = B.copy()\n",
    "    GM = m.copy()\n",
    "    gpA = [GA]\n",
    "    gpB = [GB]\n",
    "    gpM = [GM]\n",
    "    for i in range(num_levels):\n",
    "        GA = cv2.pyrDown(GA)\n",
    "        GB = cv2.pyrDown(GB)\n",
    "        GM = cv2.pyrDown(GM)\n",
    "        gpA.append(np.float32(GA))\n",
    "        gpB.append(np.float32(GB))\n",
    "        gpM.append(np.float32(GM))\n",
    "\n",
    "    # generate Laplacian Pyramids for A,B and masks\n",
    "    lpA  = [gpA[num_levels-1]] # the bottom of the Lap-pyr holds the last (smallest) Gauss level\n",
    "    lpB  = [gpB[num_levels-1]]\n",
    "    gpMr = [gpM[num_levels-1]]\n",
    "    for i in range(num_levels-1,0,-1):\n",
    "        # Laplacian: subtract upscaled version of lower level from current level\n",
    "        # to get the high frequencies\n",
    "        LA = np.subtract(gpA[i-1], cv2.pyrUp(gpA[i]))\n",
    "        LB = np.subtract(gpB[i-1], cv2.pyrUp(gpB[i]))\n",
    "        lpA.append(LA)\n",
    "        lpB.append(LB)\n",
    "        gpMr.append(gpM[i-1]) # also reverse the masks\n",
    "\n",
    "    # Now blend images according to mask in each level\n",
    "    LS = []\n",
    "    for la,lb,gm in zip(lpA,lpB,gpMr):\n",
    "        gm = gm[:,:,np.newaxis]\n",
    "        ls = la * gm + lb * (1.0 - gm)\n",
    "        LS.append(ls)\n",
    "\n",
    "    # now reconstruct\n",
    "    ls_ = LS[0]\n",
    "    for i in range(1,num_levels):\n",
    "        ls_ = cv2.pyrUp(ls_)\n",
    "        ls_ = cv2.add(ls_, LS[i])\n",
    "    return ls_\n",
    "\n",
    "def load_model(args, device):\n",
    "    D_Net = load_DNet(args).to(device)\n",
    "    model = load_network(args).to(device)\n",
    "    return D_Net, model\n",
    "\n",
    "def normalize_kp(kp_source, kp_driving, kp_driving_initial, adapt_movement_scale=False,\n",
    "                 use_relative_movement=False, use_relative_jacobian=False):\n",
    "    if adapt_movement_scale:\n",
    "        source_area = ConvexHull(kp_source['value'][0].data.cpu().numpy()).volume\n",
    "        driving_area = ConvexHull(kp_driving_initial['value'][0].data.cpu().numpy()).volume\n",
    "        adapt_movement_scale = np.sqrt(source_area) / np.sqrt(driving_area)\n",
    "    else:\n",
    "        adapt_movement_scale = 1\n",
    "\n",
    "    kp_new = {k: v for k, v in kp_driving.items()}\n",
    "    if use_relative_movement:\n",
    "        kp_value_diff = (kp_driving['value'] - kp_driving_initial['value'])\n",
    "        kp_value_diff *= adapt_movement_scale\n",
    "        kp_new['value'] = kp_value_diff + kp_source['value']\n",
    "\n",
    "        if use_relative_jacobian:\n",
    "            jacobian_diff = torch.matmul(kp_driving['jacobian'], torch.inverse(kp_driving_initial['jacobian']))\n",
    "            kp_new['jacobian'] = torch.matmul(jacobian_diff, kp_source['jacobian'])\n",
    "    return kp_new\n",
    "\n",
    "def load_face3d_net(ckpt_path, device):\n",
    "    net_recon = networks.define_net_recon(net_recon='resnet50', use_last_fc=False, init_path='').to(device)\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)    \n",
    "    net_recon.load_state_dict(checkpoint['net_recon'])\n",
    "    net_recon.eval()\n",
    "    return net_recon"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
